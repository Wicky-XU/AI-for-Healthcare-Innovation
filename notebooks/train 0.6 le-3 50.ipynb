{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e10a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID-19 Image Classification using VGG16 - 50 Epochs Version\n",
    "# This notebook is designed for comprehensive training with 50 epochs\n",
    "# Updated for new project structure with enhanced training configuration\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 1: Import Libraries and Setup\n",
    "# =============================================================================\n",
    "\n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# GPU Configuration\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"✅ GPU configuration completed. Found {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n",
    "\n",
    "print(\"✅ All libraries imported successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 2: Enhanced Configuration for 50 Epochs Training\n",
    "# =============================================================================\n",
    "\n",
    "# Path Configuration - Updated for new project structure\n",
    "PROJECT_ROOT = r'C:\\Users\\29873\\code\\Summer-projects\\lung-cnn'\n",
    "TRAIN_PATH = os.path.join(PROJECT_ROOT, 'data', 'train_covid19')\n",
    "TEST_PATH = os.path.join(PROJECT_ROOT, 'data', 'test_healthcare')\n",
    "\n",
    "# Output directories (will be created in project root for compatibility)\n",
    "PROCESSED_TRAIN_DIR = os.path.join(PROJECT_ROOT, 'Train_covid_50')\n",
    "PROCESSED_VAL_DIR = os.path.join(PROJECT_ROOT, 'Val_covid_50')\n",
    "\n",
    "# Model save path\n",
    "MODEL_SAVE_PATH = os.path.join(PROJECT_ROOT, 'models')\n",
    "RESULTS_PATH = os.path.join(PROJECT_ROOT, 'results')\n",
    "\n",
    "# Enhanced Training Configuration for 50 Epochs\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 50  # Extended training\n",
    "LEARNING_RATE = 1e-3\n",
    "RANDOM_SEED = 100\n",
    "TRAIN_VAL_SPLIT = 0.6  # 60% training, 40% validation\n",
    "\n",
    "# Enhanced callbacks configuration\n",
    "EARLY_STOPPING_PATIENCE = 10  # Increased patience for longer training\n",
    "REDUCE_LR_PATIENCE = 5\n",
    "MIN_LR = 1e-7\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(RESULTS_PATH, 'plots'), exist_ok=True)\n",
    "\n",
    "print(\"✅ Enhanced configuration completed for 50 epochs training\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Training path: {TRAIN_PATH}\")\n",
    "print(f\"Testing path: {TEST_PATH}\")\n",
    "print(f\"Model save path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"Results path: {RESULTS_PATH}\")\n",
    "print(f\"Image size: {IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Number of epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 3: Enhanced Data Structure Analysis and Validation\n",
    "# =============================================================================\n",
    "\n",
    "def check_data_structure():\n",
    "    \"\"\"\n",
    "    Analyze and validate the data structure with enhanced reporting\n",
    "    Returns True if data structure is valid, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA STRUCTURE ANALYSIS - 50 EPOCHS VERSION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check training data\n",
    "    if os.path.exists(TRAIN_PATH):\n",
    "        print(f\"✅ Training data path exists: {TRAIN_PATH}\")\n",
    "        subdirs = [d for d in os.listdir(TRAIN_PATH) if os.path.isdir(os.path.join(TRAIN_PATH, d))]\n",
    "        print(f\"Training data subdirectories: {subdirs}\")\n",
    "        \n",
    "        if len(subdirs) != 2:\n",
    "            print(f\"⚠️ Expected 2 class directories, found {len(subdirs)}\")\n",
    "        \n",
    "        total_train_images = 0\n",
    "        class_distribution = {}\n",
    "        \n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(TRAIN_PATH, subdir)\n",
    "            img_files = [f for f in os.listdir(subdir_path) \n",
    "                        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "            img_count = len(img_files)\n",
    "            total_train_images += img_count\n",
    "            class_distribution[subdir] = img_count\n",
    "            print(f\"  {subdir}: {img_count} images\")\n",
    "        \n",
    "        print(f\"Total training images: {total_train_images}\")\n",
    "        \n",
    "        # Check class balance\n",
    "        if len(class_distribution) == 2:\n",
    "            class_counts = list(class_distribution.values())\n",
    "            balance_ratio = min(class_counts) / max(class_counts)\n",
    "            print(f\"Class balance ratio: {balance_ratio:.3f}\")\n",
    "            if balance_ratio < 0.5:\n",
    "                print(\"⚠️ Significant class imbalance detected. Consider data augmentation.\")\n",
    "            else:\n",
    "                print(\"✅ Reasonable class balance detected.\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Training data path does not exist: {TRAIN_PATH}\")\n",
    "        return False\n",
    "    \n",
    "    # Check testing data with enhanced analysis\n",
    "    if os.path.exists(TEST_PATH):\n",
    "        print(f\"✅ Testing data path exists: {TEST_PATH}\")\n",
    "        \n",
    "        # Check for subdirectories and files\n",
    "        items = os.listdir(TEST_PATH)\n",
    "        subdirs = [d for d in items if os.path.isdir(os.path.join(TEST_PATH, d))]\n",
    "        files = [f for f in items if os.path.isfile(os.path.join(TEST_PATH, f)) \n",
    "                and f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "        \n",
    "        print(f\"Testing data subdirectories: {subdirs}\")\n",
    "        print(f\"Testing data root directory images: {len(files)}\")\n",
    "        \n",
    "        total_test_images = len(files)\n",
    "        if subdirs:\n",
    "            for subdir in subdirs:\n",
    "                subdir_path = os.path.join(TEST_PATH, subdir)\n",
    "                if os.path.isdir(subdir_path):\n",
    "                    img_files = [f for f in os.listdir(subdir_path) \n",
    "                               if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "                    img_count = len(img_files)\n",
    "                    total_test_images += img_count\n",
    "                    print(f\"  {subdir}: {img_count} images\")\n",
    "        \n",
    "        print(f\"Total testing images: {total_test_images}\")\n",
    "        \n",
    "        # Recommend train/test ratio\n",
    "        if total_train_images > 0:\n",
    "            test_ratio = total_test_images / (total_train_images + total_test_images)\n",
    "            print(f\"Train/Test ratio: {1-test_ratio:.3f}/{test_ratio:.3f}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ Testing data path does not exist: {TEST_PATH}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    return True\n",
    "\n",
    "# Run enhanced data structure check\n",
    "if not check_data_structure():\n",
    "    print(\"❌ Please check your data paths and structure!\")\n",
    "    raise SystemExit(\"Data structure validation failed\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 4: Enhanced Directory Setup and Data Splitting\n",
    "# =============================================================================\n",
    "\n",
    "def setup_directories():\n",
    "    \"\"\"\n",
    "    Create and clean up training and validation directories for 50-epoch version\n",
    "    \"\"\"\n",
    "    print(\"Setting up directories for 50-epoch training...\")\n",
    "    \n",
    "    # Remove existing directories\n",
    "    for dir_path in [PROCESSED_TRAIN_DIR, PROCESSED_VAL_DIR]:\n",
    "        if os.path.exists(dir_path):\n",
    "            shutil.rmtree(dir_path)\n",
    "            print(f\"Removed existing directory: {dir_path}\")\n",
    "    \n",
    "    # Create new directories\n",
    "    for dir_path in [PROCESSED_TRAIN_DIR, PROCESSED_VAL_DIR]:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        os.makedirs(os.path.join(dir_path, 'yes'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(dir_path, 'no'), exist_ok=True)\n",
    "        print(f\"Created directory: {dir_path}\")\n",
    "    \n",
    "    print(\"✅ Directory setup completed for 50-epoch version\")\n",
    "\n",
    "def split_data():\n",
    "    \"\"\"\n",
    "    Split training data into training and validation sets with enhanced logging\n",
    "    Returns True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    print(\"\\n📈 Next Steps:\")\n",
    "print(\"1. Run the 10-epoch version for comparison\")\n",
    "print(\"2. Use the modular code in src/ for production deployment\")\n",
    "print(\"3. Implement model comparison and ensemble methods\")\n",
    "print(\"4. Consider creating a web interface for predictions\")\n",
    "print(\"5. Set up automated model retraining pipeline\")\n",
    "\n",
    "print(\"\\n💡 Usage Examples:\")\n",
    "print(\"# Load the trained model for inference:\")\n",
    "print(\"from tensorflow.keras.models import load_model\")\n",
    "print(f\"model = load_model('{model_filename}')\")\n",
    "print(\"\")\n",
    "print(\"# Use the enhanced prediction pipeline:\")\n",
    "print(\"python src/main.py --mode predict --version 50_epochs\")\n",
    "print(\"\")\n",
    "print(\"# Compare with 10-epoch version:\")\n",
    "print(\"python src/main.py --mode compare\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"🎉 50-EPOCH TRAINING SESSION COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*80)\n",
    "print(\"Your model is ready for advanced deployment and clinical evaluation.\")\n",
    "print(\"Remember to validate performance on independent test sets before clinical use.\")\n",
    "print(\"\\n\")Splitting data into training and validation sets (50-epoch version)...\")\n",
    "    \n",
    "    # Get class folders\n",
    "    class_folders = [d for d in os.listdir(TRAIN_PATH) \n",
    "                    if os.path.isdir(os.path.join(TRAIN_PATH, d))]\n",
    "    \n",
    "    if len(class_folders) != 2:\n",
    "        print(f\"⚠️ Found {len(class_folders)} class folders, expected 2\")\n",
    "        print(f\"Class folders: {class_folders}\")\n",
    "        \n",
    "        # Create mapping for non-standard folder names\n",
    "        if len(class_folders) == 2:\n",
    "            # Assume first folder is 'no' and second is 'yes' or map based on folder names\n",
    "            if 'no' in class_folders and 'yes' in class_folders:\n",
    "                class_mapping = {'no': 'no', 'yes': 'yes'}\n",
    "            else:\n",
    "                class_mapping = {class_folders[0]: 'no', class_folders[1]: 'yes'}\n",
    "            print(f\"Class mapping: {class_mapping}\")\n",
    "        else:\n",
    "            print(\"❌ Please ensure training data has exactly 2 class folders!\")\n",
    "            return False\n",
    "    else:\n",
    "        # Standard case with 'yes' and 'no' folders\n",
    "        class_mapping = {folder: folder for folder in class_folders}\n",
    "    \n",
    "    total_train = 0\n",
    "    total_val = 0\n",
    "    split_summary = {}\n",
    "    \n",
    "    for original_class, target_class in class_mapping.items():\n",
    "        class_path = os.path.join(TRAIN_PATH, original_class)\n",
    "        files = [f for f in os.listdir(class_path) \n",
    "                if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "        \n",
    "        if len(files) == 0:\n",
    "            print(f\"⚠️ No image files found in {original_class} folder\")\n",
    "            continue\n",
    "            \n",
    "        # Shuffle and split data\n",
    "        random.shuffle(files)\n",
    "        split_point = int(TRAIN_VAL_SPLIT * len(files))\n",
    "        \n",
    "        # Copy training data\n",
    "        train_files = files[:split_point]\n",
    "        for file_name in train_files:\n",
    "            src = os.path.join(class_path, file_name)\n",
    "            dst = os.path.join(PROCESSED_TRAIN_DIR, target_class, file_name)\n",
    "            shutil.copy2(src, dst)\n",
    "        \n",
    "        # Copy validation data\n",
    "        val_files = files[split_point:]\n",
    "        for file_name in val_files:\n",
    "            src = os.path.join(class_path, file_name)\n",
    "            dst = os.path.join(PROCESSED_VAL_DIR, target_class, file_name)\n",
    "            shutil.copy2(src, dst)\n",
    "        \n",
    "        total_train += len(train_files)\n",
    "        total_val += len(val_files)\n",
    "        \n",
    "        split_summary[original_class] = {\n",
    "            'total': len(files),\n",
    "            'train': len(train_files),\n",
    "            'val': len(val_files),\n",
    "            'train_ratio': len(train_files) / len(files),\n",
    "            'val_ratio': len(val_files) / len(files)\n",
    "        }\n",
    "        \n",
    "        print(f\"{original_class} -> {target_class}: {len(train_files)} training, {len(val_files)} validation\")\n",
    "    \n",
    "    print(f\"\\nDetailed split summary:\")\n",
    "    for class_name, stats in split_summary.items():\n",
    "        print(f\"  {class_name}: {stats['total']} total | \"\n",
    "              f\"{stats['train']} train ({stats['train_ratio']:.1%}) | \"\n",
    "              f\"{stats['val']} val ({stats['val_ratio']:.1%})\")\n",
    "    \n",
    "    print(f\"\\nOverall: {total_train} training images, {total_val} validation images\")\n",
    "    print(\"✅ Data splitting completed successfully for 50-epoch version\")\n",
    "    return True\n",
    "\n",
    "# Execute directory setup and data splitting\n",
    "setup_directories()\n",
    "if not split_data():\n",
    "    raise SystemExit(\"Data splitting failed\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 5: Enhanced Data Visualization\n",
    "# =============================================================================\n",
    "\n",
    "def plot_samples(img_path, n=25, title=\"Sample Images\"):\n",
    "    \"\"\"\n",
    "    Display sample images from the dataset with enhanced layout\n",
    "    \n",
    "    Args:\n",
    "        img_path (str): Path to image directory\n",
    "        n (int): Number of images to display\n",
    "        title (str): Title for the plot\n",
    "    \"\"\"\n",
    "    files_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    # Collect all image files\n",
    "    for root, dirs, files in os.walk(img_path):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                files_list.append(os.path.join(root, file))\n",
    "                label = os.path.basename(root)\n",
    "                labels_list.append(label)\n",
    "    \n",
    "    if not files_list:\n",
    "        print(f\"No image files found in {img_path}\")\n",
    "        return\n",
    "    \n",
    "    # Randomly select images\n",
    "    combined = list(zip(files_list, labels_list))\n",
    "    random.shuffle(combined)\n",
    "    files_list, labels_list = zip(*combined)\n",
    "    \n",
    "    # Limit number of images to display\n",
    "    n = min(n, len(files_list))\n",
    "    cols = 5\n",
    "    rows = (n + cols - 1) // cols\n",
    "    \n",
    "    plt.figure(figsize=(20, 4 * rows))\n",
    "    plt.suptitle(title, fontsize=18, fontweight='bold')\n",
    "    \n",
    "    for i in range(n):\n",
    "        file_path, label = files_list[i], labels_list[i]\n",
    "        \n",
    "        # Read and display image\n",
    "        img = cv2.imread(file_path)\n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            plt.subplot(rows, cols, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f'Class: {label}', fontsize=12, fontweight='bold')\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_class_distribution():\n",
    "    \"\"\"Plot class distribution for both training and validation sets\"\"\"\n",
    "    train_classes = {'yes': 0, 'no': 0}\n",
    "    val_classes = {'yes': 0, 'no': 0}\n",
    "    \n",
    "    # Count training images\n",
    "    for class_name in ['yes', 'no']:\n",
    "        train_path = os.path.join(PROCESSED_TRAIN_DIR, class_name)\n",
    "        val_path = os.path.join(PROCESSED_VAL_DIR, class_name)\n",
    "        \n",
    "        if os.path.exists(train_path):\n",
    "            train_classes[class_name] = len([f for f in os.listdir(train_path) \n",
    "                                           if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "        \n",
    "        if os.path.exists(val_path):\n",
    "            val_classes[class_name] = len([f for f in os.listdir(val_path) \n",
    "                                         if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))])\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Training distribution\n",
    "    ax1.bar(train_classes.keys(), train_classes.values(), color=['lightcoral', 'lightblue'])\n",
    "    ax1.set_title('Training Set Class Distribution', fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Images')\n",
    "    for i, (k, v) in enumerate(train_classes.items()):\n",
    "        ax1.text(i, v + max(train_classes.values()) * 0.01, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Validation distribution\n",
    "    ax2.bar(val_classes.keys(), val_classes.values(), color=['lightcoral', 'lightblue'])\n",
    "    ax2.set_title('Validation Set Class Distribution', fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Images')\n",
    "    for i, (k, v) in enumerate(val_classes.items()):\n",
    "        ax2.text(i, v + max(val_classes.values()) * 0.01, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return train_classes, val_classes\n",
    "\n",
    "# Display enhanced visualizations\n",
    "print(\"\\nDisplaying sample training images...\")\n",
    "plot_samples(PROCESSED_TRAIN_DIR, n=25, title=\"Training Dataset Sample Images - 50 Epochs Version\")\n",
    "\n",
    "print(\"\\nAnalyzing class distribution...\")\n",
    "train_dist, val_dist = plot_class_distribution()\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 6: Enhanced Data Generators Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Enhanced training data generator with more aggressive augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,          # Increased from 20\n",
    "    width_shift_range=0.25,     # Increased from 0.2\n",
    "    height_shift_range=0.25,    # Increased from 0.2\n",
    "    shear_range=0.25,          # Increased from 0.2\n",
    "    zoom_range=0.25,           # Increased from 0.2\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False,        # Keep False for medical images\n",
    "    brightness_range=[0.8, 1.2], # New: brightness variation\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "# Validation data generator (no augmentation)\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "print(\"Enhanced data generators created with the following augmentations:\")\n",
    "print(\"- Rotation: ±30 degrees (increased)\")\n",
    "print(\"- Width/Height shift: ±25% (increased)\")\n",
    "print(\"- Shear transformation: 25% (increased)\")\n",
    "print(\"- Zoom: ±25% (increased)\")\n",
    "print(\"- Horizontal flip: Yes\")\n",
    "print(\"- Brightness variation: 0.8-1.2 (new)\")\n",
    "print(\"- Vertical flip: No (medical image consideration)\")\n",
    "\n",
    "# Create enhanced data generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    PROCESSED_TRAIN_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    PROCESSED_VAL_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='binary',\n",
    "    seed=RANDOM_SEED,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Enhanced data generators created successfully\")\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {validation_generator.samples}\")\n",
    "print(f\"Number of classes: {len(train_generator.class_indices)}\")\n",
    "print(f\"Class indices: {train_generator.class_indices}\")\n",
    "\n",
    "# Calculate expected training time\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = validation_generator.samples // BATCH_SIZE\n",
    "total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "\n",
    "print(f\"\\nTraining details:\")\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Estimated time per epoch: ~{steps_per_epoch * 2}s (assuming 2s/step)\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 7: Enhanced Model Architecture\n",
    "# =============================================================================\n",
    "\n",
    "def create_enhanced_model():\n",
    "    \"\"\"\n",
    "    Create and compile an enhanced CNN model based on VGG16 for 50-epoch training\n",
    "    \n",
    "    Returns:\n",
    "        tensorflow.keras.Model: Compiled model with enhanced architecture\n",
    "    \"\"\"\n",
    "    # Load pre-trained VGG16 model\n",
    "    base_model = VGG16(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=IMG_SIZE + (3,)\n",
    "    )\n",
    "    \n",
    "    # Create enhanced custom classifier on top\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dropout(0.6),  # Increased dropout for longer training\n",
    "        Dense(256, activation='relu'),  # Increased units\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.4),  # Additional dropout layer\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),  # Additional layer\n",
    "        Dropout(0.2),\n",
    "        Dense(1, activation='sigmoid')  # Binary classification\n",
    "    ])\n",
    "    \n",
    "    # Freeze pre-trained layers initially\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Compile model with enhanced configuration\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']  # Additional metrics\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create enhanced model\n",
    "print(\"Creating enhanced model architecture for 50-epoch training...\")\n",
    "model = create_enhanced_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENHANCED MODEL ARCHITECTURE - 50 EPOCHS VERSION\")\n",
    "print(\"=\"*70)\n",
    "model.summary()\n",
    "\n",
    "# Calculate total parameters\n",
    "trainable_params = sum([np.prod(v.get_shape().as_list()) for v in model.trainable_variables])\n",
    "non_trainable_params = sum([np.prod(v.get_shape().as_list()) for v in model.non_trainable_variables])\n",
    "\n",
    "print(f\"\\nModel parameters:\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Non-trainable parameters: {non_trainable_params:,}\")\n",
    "print(f\"Total parameters: {trainable_params + non_trainable_params:,}\")\n",
    "\n",
    "print(f\"\\nEnhanced features for 50-epoch training:\")\n",
    "print(\"- Deeper classifier network (4 dense layers)\")\n",
    "print(\"- Increased dropout rates for better regularization\")\n",
    "print(\"- Additional metrics: precision and recall\")\n",
    "print(\"- Enhanced data augmentation\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 8: Enhanced Training Configuration and Callbacks\n",
    "# =============================================================================\n",
    "\n",
    "# Define enhanced callbacks for 50-epoch training\n",
    "callbacks = [\n",
    "    # Early stopping with increased patience\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=EARLY_STOPPING_PATIENCE,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='max'\n",
    "    ),\n",
    "    \n",
    "    # Model checkpoint to save best model\n",
    "    ModelCheckpoint(\n",
    "        filepath=os.path.join(MODEL_SAVE_PATH, 'best_model_50epochs.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_weights_only=False\n",
    "    ),\n",
    "    \n",
    "    # Learning rate reduction\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=REDUCE_LR_PATIENCE,\n",
    "        min_lr=MIN_LR,\n",
    "        verbose=1,\n",
    "        cooldown=2\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Enhanced training configuration for 50 epochs:\")\n",
    "print(f\"- Optimizer: Adam (learning_rate={LEARNING_RATE})\")\n",
    "print(f\"- Loss function: Binary crossentropy\")\n",
    "print(f\"- Metrics: Accuracy, Precision, Recall\")\n",
    "print(f\"- Early stopping: val_accuracy (patience={EARLY_STOPPING_PATIENCE})\")\n",
    "print(f\"- Model checkpoint: best_model_50epochs.h5\")\n",
    "print(f\"- Learning rate reduction: factor=0.2, patience={REDUCE_LR_PATIENCE}\")\n",
    "print(f\"- Number of epochs: {NUM_EPOCHS}\")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = validation_generator.samples // BATCH_SIZE\n",
    "\n",
    "print(f\"- Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"- Validation steps: {validation_steps}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 9: Enhanced Model Training (50 epochs with detailed monitoring)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STARTING ENHANCED MODEL TRAINING - 50 EPOCHS VERSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Enhanced callback for detailed epoch monitoring\n",
    "class EnhancedVerboseCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch_start_time = None\n",
    "    \n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        import time\n",
    "        self.epoch_start_time = time.time()\n",
    "        print(f\"\\n--- Epoch {epoch + 1}/{NUM_EPOCHS} ---\")\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        import time\n",
    "        epoch_time = time.time() - self.epoch_start_time\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS} completed in {epoch_time:.1f}s\")\n",
    "        print(f\"  Training   - Loss: {logs['loss']:.4f}, Accuracy: {logs['accuracy']:.4f}\")\n",
    "        print(f\"  Validation - Loss: {logs['val_loss']:.4f}, Accuracy: {logs['val_accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {logs.get('precision', 'N/A'):.4f}, Recall: {logs.get('recall', 'N/A'):.4f}\")\n",
    "        \n",
    "        # Progress indicator\n",
    "        progress = (epoch + 1) / NUM_EPOCHS * 100\n",
    "        print(f\"  Progress: {progress:.1f}% complete\")\n",
    "\n",
    "# Add enhanced callback\n",
    "callbacks.append(EnhancedVerboseCallback())\n",
    "\n",
    "# Train the enhanced model\n",
    "print(\"Starting 50-epoch training session...\")\n",
    "print(\"This may take several hours depending on your hardware.\")\n",
    "print(\"Monitor GPU/CPU usage and ensure adequate cooling.\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1  # Show progress bar for each epoch\n",
    ")\n",
    "\n",
    "print(\"\\n✅ 50-epoch training completed successfully!\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 10: Enhanced Training Results Visualization\n",
    "# =============================================================================\n",
    "\n",
    "def plot_enhanced_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot comprehensive training history for 50-epoch training\n",
    "    \n",
    "    Args:\n",
    "        history: Training history object\n",
    "    \"\"\"\n",
    "    # Create a comprehensive figure with multiple subplots\n",
    "    fig = plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "    \n",
    "    # 1. Accuracy plot\n",
    "    plt.subplot(3, 3, 1)\n",
    "    plt.plot(epochs_range, history.history['accuracy'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    plt.plot(epochs_range, history.history['val_accuracy'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    plt.title('Model Accuracy - 50 Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Loss plot\n",
    "    plt.subplot(3, 3, 2)\n",
    "    plt.plot(epochs_range, history.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    plt.plot(epochs_range, history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    plt.title('Model Loss - 50 Epochs', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Precision plot (if available)\n",
    "    if 'precision' in history.history:\n",
    "        plt.subplot(3, 3, 3)\n",
    "        plt.plot(epochs_range, history.history['precision'], 'g-', label='Training Precision', linewidth=2)\n",
    "        if 'val_precision' in history.history:\n",
    "            plt.plot(epochs_range, history.history['val_precision'], 'orange', label='Validation Precision', linewidth=2)\n",
    "        plt.title('Model Precision - 50 Epochs', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Recall plot (if available)\n",
    "    if 'recall' in history.history:\n",
    "        plt.subplot(3, 3, 4)\n",
    "        plt.plot(epochs_range, history.history['recall'], 'purple', label='Training Recall', linewidth=2)\n",
    "        if 'val_recall' in history.history:\n",
    "            plt.plot(epochs_range, history.history['val_recall'], 'brown', label='Validation Recall', linewidth=2)\n",
    "        plt.title('Model Recall - 50 Epochs', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Recall')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Learning rate plot (if available in callbacks)\n",
    "    plt.subplot(3, 3, 5)\n",
    "    if hasattr(model.optimizer, 'learning_rate'):\n",
    "        # This is a simplified representation - actual LR changes would need callback logging\n",
    "        plt.plot(epochs_range, [LEARNING_RATE] * len(epochs_range), 'k--', label='Learning Rate')\n",
    "        plt.title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Accuracy comparison bar chart\n",
    "    plt.subplot(3, 3, 6)\n",
    "    final_train_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    metrics = ['Final Train', 'Final Val', 'Best Val']\n",
    "    values = [final_train_acc, final_val_acc, best_val_acc]\n",
    "    colors = ['blue', 'red', 'green']\n",
    "    \n",
    "    bars = plt.bar(metrics, values, color=colors, alpha=0.7)\n",
    "    plt.title('Accuracy Summary', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    for bar, value in zip(bars, values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, value + 0.01, \n",
    "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 7. Loss comparison bar chart\n",
    "    plt.subplot(3, 3, 7)\n",
    "    final_train_loss = history.history['loss'][-1]\n",
    "    final_val_loss = history.history['val_loss'][-1]\n",
    "    min_val_loss = min(history.history['val_loss'])\n",
    "    \n",
    "    loss_metrics = ['Final Train', 'Final Val', 'Min Val']\n",
    "    loss_values = [final_train_loss, final_val_loss, min_val_loss]\n",
    "    \n",
    "    bars = plt.bar(loss_metrics, loss_values, color=colors, alpha=0.7)\n",
    "    plt.title('Loss Summary', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Loss')\n",
    "    \n",
    "    for bar, value in zip(bars, loss_values):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, value + max(loss_values) * 0.02, \n",
    "                f'{value:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 8. Training progress over time\n",
    "    plt.subplot(3, 3, 8)\n",
    "    # Calculate moving averages for smoother visualization\n",
    "    window_size = max(1, len(epochs_range) // 10)\n",
    "    \n",
    "    def moving_average(data, window_size):\n",
    "        return [sum(data[max(0, i-window_size):i+1]) / min(i+1, window_size) for i in range(len(data))]\n",
    "    \n",
    "    smooth_train_acc = moving_average(history.history['accuracy'], window_size)\n",
    "    smooth_val_acc = moving_average(history.history['val_accuracy'], window_size)\n",
    "    \n",
    "    plt.plot(epochs_range, smooth_train_acc, 'b-', label='Smooth Train Acc', linewidth=2)\n",
    "    plt.plot(epochs_range, smooth_val_acc, 'r-', label='Smooth Val Acc', linewidth=2)\n",
    "    plt.title('Smoothed Accuracy Trends', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 9. Overfitting analysis\n",
    "    plt.subplot(3, 3, 9)\n",
    "    acc_diff = [train - val for train, val in zip(history.history['accuracy'], history.history['val_accuracy'])]\n",
    "    plt.plot(epochs_range, acc_diff, 'purple', linewidth=2, label='Train - Val Accuracy')\n",
    "    plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    plt.title('Overfitting Analysis', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy Difference')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(RESULTS_PATH, 'plots', 'training_history_50epochs.png')\n",
    "    fig.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Training plots saved to: {plot_path}\")\n",
    "\n",
    "# Plot enhanced training results\n",
    "print(\"Generating comprehensive training results visualization...\")\n",
    "plot_enhanced_training_history(history)\n",
    "\n",
    "# Enhanced training summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"COMPREHENSIVE TRAINING SUMMARY - 50 EPOCHS VERSION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "epochs_completed = len(history.history['accuracy'])\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "best_val_acc_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "min_val_loss_epoch = np.argmin(history.history['val_loss']) + 1\n",
    "min_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "print(f\"Training Progress:\")\n",
    "print(f\"  Epochs completed: {epochs_completed}/{NUM_EPOCHS}\")\n",
    "print(f\"  Early stopping: {'Yes' if epochs_completed < NUM_EPOCHS else 'No'}\")\n",
    "\n",
    "print(f\"\\nFinal Performance:\")\n",
    "print(f\"  Training accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"  Validation accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"  Training loss: {final_train_loss:.4f}\")\n",
    "print(f\"  Validation loss: {final_val_loss:.4f}\")\n",
    "\n",
    "print(f\"\\nBest Performance:\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.4f} (epoch {best_val_acc_epoch})\")\n",
    "print(f\"  Minimum validation loss: {min_val_loss:.4f} (epoch {min_val_loss_epoch})\")\n",
    "\n",
    "# Overfitting analysis\n",
    "accuracy_gap = final_train_acc - final_val_acc\n",
    "if accuracy_gap > 0.1:\n",
    "    print(f\"\\n⚠️ Overfitting detected! Training accuracy exceeds validation by {accuracy_gap:.3f}\")\n",
    "    print(\"   Consider: more regularization, data augmentation, or early stopping\")\n",
    "elif accuracy_gap < 0:\n",
    "    print(f\"\\n📈 Underfitting possible. Validation accuracy exceeds training by {abs(accuracy_gap):.3f}\")\n",
    "    print(\"   Consider: reducing regularization or training longer\")\n",
    "else:\n",
    "    print(f\"\\n✅ Good fit! Accuracy gap is reasonable: {accuracy_gap:.3f}\")\n",
    "\n",
    "# Additional metrics if available\n",
    "if 'precision' in history.history:\n",
    "    final_precision = history.history['precision'][-1]\n",
    "    print(f\"  Final precision: {final_precision:.4f}\")\n",
    "\n",
    "if 'recall' in history.history:\n",
    "    final_recall = history.history['recall'][-1]\n",
    "    print(f\"  Final recall: {final_recall:.4f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 11: Enhanced Model Saving and Checkpointing\n",
    "# =============================================================================\n",
    "\n",
    "# Save the final trained model\n",
    "model_filename = os.path.join(MODEL_SAVE_PATH, 'covid_classifier_vgg16_50epochs_final.h5')\n",
    "model.save(model_filename)\n",
    "print(f\"✅ Final model saved as: {model_filename}\")\n",
    "\n",
    "# Save training history with enhanced data\n",
    "import pickle\n",
    "history_data = {\n",
    "    'history': history.history,\n",
    "    'config': {\n",
    "        'epochs': NUM_EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'learning_rate': LEARNING_RATE,\n",
    "        'train_samples': train_generator.samples,\n",
    "        'val_samples': validation_generator.samples,\n",
    "        'train_val_split': TRAIN_VAL_SPLIT,\n",
    "        'augmentation': 'enhanced',\n",
    "        'architecture': 'vgg16_enhanced'\n",
    "    },\n",
    "    'performance': {\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'best_val_acc_epoch': best_val_acc_epoch,\n",
    "        'final_val_acc': final_val_acc,\n",
    "        'min_val_loss': min_val_loss,\n",
    "        'min_val_loss_epoch': min_val_loss_epoch,\n",
    "        'final_val_loss': final_val_loss\n",
    "    }\n",
    "}\n",
    "\n",
    "history_filename = os.path.join(MODEL_SAVE_PATH, 'training_history_50epochs_enhanced.pkl')\n",
    "with open(history_filename, 'wb') as f:\n",
    "    pickle.dump(history_data, f)\n",
    "print(f\"✅ Enhanced training history saved as: {history_filename}\")\n",
    "\n",
    "# Create model summary file\n",
    "summary_filename = os.path.join(RESULTS_PATH, 'model_summary_50epochs.txt')\n",
    "with open(summary_filename, 'w') as f:\n",
    "    f.write(\"COVID-19 Classification Model - 50 Epochs Version\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    f.write(f\"Training completed: {epochs_completed}/{NUM_EPOCHS} epochs\\n\")\n",
    "    f.write(f\"Best validation accuracy: {best_val_acc:.4f} (epoch {best_val_acc_epoch})\\n\")\n",
    "    f.write(f\"Final validation accuracy: {final_val_acc:.4f}\\n\")\n",
    "    f.write(f\"Minimum validation loss: {min_val_loss:.4f} (epoch {min_val_loss_epoch})\\n\")\n",
    "    f.write(f\"Model architecture: Enhanced VGG16\\n\")\n",
    "    f.write(f\"Total parameters: {trainable_params + non_trainable_params:,}\\n\")\n",
    "    f.write(f\"Training samples: {train_generator.samples}\\n\")\n",
    "    f.write(f\"Validation samples: {validation_generator.samples}\\n\")\n",
    "\n",
    "print(f\"✅ Model summary saved as: {summary_filename}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 12: Enhanced Test Data Prediction\n",
    "# =============================================================================\n",
    "\n",
    "def predict_test_images_enhanced():\n",
    "    \"\"\"\n",
    "    Enhanced prediction on test images with detailed analysis\n",
    "    \n",
    "    Returns:\n",
    "        list: List of enhanced prediction results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ENHANCED TEST DATA PREDICTION - 50 EPOCHS VERSION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    if not os.path.exists(TEST_PATH):\n",
    "        print(f\"❌ Test data path does not exist: {TEST_PATH}\")\n",
    "        return []\n",
    "    \n",
    "    # Collect all test images with enhanced organization\n",
    "    test_image_paths = []\n",
    "    test_subdirs = []\n",
    "    \n",
    "    # Search in subdirectories\n",
    "    subdirs = [d for d in os.listdir(TEST_PATH) if os.path.isdir(os.path.join(TEST_PATH, d))]\n",
    "    if subdirs:\n",
    "        print(f\"Searching for images in subdirectories: {subdirs}\")\n",
    "        for subdir in subdirs:\n",
    "            subdir_path = os.path.join(TEST_PATH, subdir)\n",
    "            subdir_files = []\n",
    "            for file in os.listdir(subdir_path):\n",
    "                if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                    full_path = os.path.join(subdir_path, file)\n",
    "                    test_image_paths.append(full_path)\n",
    "                    test_subdirs.append(subdir)\n",
    "                    subdir_files.append(file)\n",
    "            print(f\"  {subdir}: {len(subdir_files)} images\")\n",
    "    \n",
    "    # Search in root directory\n",
    "    root_files = [f for f in os.listdir(TEST_PATH) \n",
    "                  if os.path.isfile(os.path.join(TEST_PATH, f)) \n",
    "                  and f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
    "    \n",
    "    if root_files:\n",
    "        print(f\"Found {len(root_files)} images in root directory\")\n",
    "        for file in root_files:\n",
    "            test_image_paths.append(os.path.join(TEST_PATH, file))\n",
    "            test_subdirs.append('root')\n",
    "    \n",
    "    if not test_image_paths:\n",
    "        print(\"❌ No test images found!\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"✅ Found {len(test_image_paths)} test images total\")\n",
    "    \n",
    "    # Enhanced preprocessing with batch processing\n",
    "    test_images = []\n",
    "    valid_paths = []\n",
    "    valid_subdirs = []\n",
    "    failed_images = []\n",
    "    \n",
    "    print(\"Preprocessing test images with enhanced validation...\")\n",
    "    for i, (img_path, subdir) in enumerate(tqdm(zip(test_image_paths, test_subdirs), \n",
    "                                                 desc=\"Loading images\", \n",
    "                                                 total=len(test_image_paths))):\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Enhanced preprocessing with validation\n",
    "                original_shape = img.shape\n",
    "                img_resized = cv2.resize(img, IMG_SIZE)\n",
    "                img_preprocessed = preprocess_input(img_resized)\n",
    "                \n",
    "                # Basic image quality checks\n",
    "                if np.mean(img_preprocessed) != 0:  # Check if image is not completely black\n",
    "                    test_images.append(img_preprocessed)\n",
    "                    valid_paths.append(img_path)\n",
    "                    valid_subdirs.append(subdir)\n",
    "                else:\n",
    "                    failed_images.append((img_path, \"Image appears to be corrupted\"))\n",
    "            else:\n",
    "                failed_images.append((img_path, \"Could not read image\"))\n",
    "        except Exception as e:\n",
    "            failed_images.append((img_path, f\"Error: {str(e)}\"))\n",
    "    \n",
    "    if failed_images:\n",
    "        print(f\"⚠️ Failed to process {len(failed_images)} images:\")\n",
    "        for path, reason in failed_images[:5]:  # Show first 5 failures\n",
    "            print(f\"  {os.path.basename(path)}: {reason}\")\n",
    "        if len(failed_images) > 5:\n",
    "            print(f\"  ... and {len(failed_images) - 5} more\")\n",
    "    \n",
    "    if not test_images:\n",
    "        print(\"❌ No valid test images loaded!\")\n",
    "        return []\n",
    "    \n",
    "    test_images = np.array(test_images)\n",
    "    print(f\"✅ Successfully loaded {len(test_images)} test images\")\n",
    "    print(f\"Image array shape: {test_images.shape}\")\n",
    "    \n",
    "    # Enhanced prediction with uncertainty estimation\n",
    "    print(\"Making enhanced predictions...\")\n",
    "    print(\"Using model with 50-epoch training for optimal performance...\")\n",
    "    \n",
    "    # Get predictions with additional statistics\n",
    "    predictions = model.predict(test_images, verbose=1, batch_size=BATCH_SIZE)\n",
    "    predicted_classes = (predictions > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Calculate prediction confidence and uncertainty\n",
    "    prediction_probs = predictions.flatten()\n",
    "    uncertainties = 1 - np.abs(prediction_probs - 0.5) * 2  # Distance from decision boundary\n",
    "    \n",
    "    # Get class name mapping\n",
    "    class_names = {v: k for k, v in train_generator.class_indices.items()}\n",
    "    \n",
    "    # Prepare enhanced results\n",
    "    results = []\n",
    "    for i, (img_path, subdir, pred_prob, pred_class, uncertainty) in enumerate(\n",
    "        zip(valid_paths, valid_subdirs, prediction_probs, predicted_classes, uncertainties)):\n",
    "        \n",
    "        filename = os.path.basename(img_path)\n",
    "        class_name = class_names[pred_class]\n",
    "        confidence = pred_prob if pred_class == 1 else 1 - pred_prob\n",
    "        \n",
    "        results.append({\n",
    "            'filename': filename,\n",
    "            'path': img_path,\n",
    "            'subdirectory': subdir,\n",
    "            'prediction': class_name,\n",
    "            'confidence': confidence,\n",
    "            'uncertainty': uncertainty,\n",
    "            'raw_probability': pred_prob,\n",
    "            'predicted_class': pred_class\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute enhanced test prediction\n",
    "test_results = predict_test_images_enhanced()\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 13: Comprehensive Results Analysis and Visualization\n",
    "# =============================================================================\n",
    "\n",
    "if test_results:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPREHENSIVE PREDICTION RESULTS - 50 EPOCHS VERSION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Enhanced sorting and analysis\n",
    "    test_results.sort(key=lambda x: x['confidence'], reverse=True)\n",
    "    \n",
    "    # Detailed statistics\n",
    "    total_predictions = len(test_results)\n",
    "    class_counts = {}\n",
    "    confidence_stats = {'yes': [], 'no': []}\n",
    "    uncertainty_stats = {'yes': [], 'no': []}\n",
    "    subdir_stats = {}\n",
    "    \n",
    "    for result in test_results:\n",
    "        pred = result['prediction']\n",
    "        subdir = result['subdirectory']\n",
    "        \n",
    "        # Class statistics\n",
    "        class_counts[pred] = class_counts.get(pred, 0) + 1\n",
    "        confidence_stats[pred].append(result['confidence'])\n",
    "        uncertainty_stats[pred].append(result['uncertainty'])\n",
    "        \n",
    "        # Subdirectory statistics\n",
    "        if subdir not in subdir_stats:\n",
    "            subdir_stats[subdir] = {'yes': 0, 'no': 0, 'total': 0}\n",
    "        subdir_stats[subdir][pred] += 1\n",
    "        subdir_stats[subdir]['total'] += 1\n",
    "    \n",
    "    # Display top predictions with enhanced information\n",
    "    print(\"\\n📊 Top predictions (by confidence):\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Filename':<35} {'Prediction':<10} {'Confidence':<12} {'Uncertainty':<12} {'Source':<15}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for i, result in enumerate(test_results[:25]):  # Show top 25\n",
    "        print(f\"{result['filename']:<35} {result['prediction']:<10} \"\n",
    "              f\"{result['confidence']:.4f}      {result['uncertainty']:.4f}      \"\n",
    "              f\"{result['subdirectory']:<15}\")\n",
    "    \n",
    "    if len(test_results) > 25:\n",
    "        print(f\"... and {len(test_results) - 25} more results\")\n",
    "    \n",
    "    # Enhanced statistical analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DETAILED PREDICTION STATISTICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Class distribution analysis\n",
    "    print(\"\\n🎯 Class Distribution:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        percentage = (count / total_predictions) * 100\n",
    "        avg_confidence = np.mean(confidence_stats[class_name])\n",
    "        avg_uncertainty = np.mean(uncertainty_stats[class_name])\n",
    "        std_confidence = np.std(confidence_stats[class_name])\n",
    "        \n",
    "        print(f\"  {class_name.upper()}: {count:3d} images ({percentage:5.1f}%)\")\n",
    "        print(f\"    Average confidence: {avg_confidence:.4f} (±{std_confidence:.4f})\")\n",
    "        print(f\"    Average uncertainty: {avg_uncertainty:.4f}\")\n",
    "    \n",
    "    # Subdirectory analysis\n",
    "    if len(subdir_stats) > 1:\n",
    "        print(\"\\n📁 Subdirectory Analysis:\")\n",
    "        for subdir, stats in subdir_stats.items():\n",
    "            if stats['total'] > 0:\n",
    "                yes_pct = (stats['yes'] / stats['total']) * 100\n",
    "                no_pct = (stats['no'] / stats['total']) * 100\n",
    "                print(f\"  {subdir}: {stats['total']} images\")\n",
    "                print(f\"    YES: {stats['yes']} ({yes_pct:.1f}%), NO: {stats['no']} ({no_pct:.1f}%)\")\n",
    "    \n",
    "    # Overall confidence statistics\n",
    "    all_confidences = [r['confidence'] for r in test_results]\n",
    "    all_uncertainties = [r['uncertainty'] for r in test_results]\n",
    "    \n",
    "    print(f\"\\n📈 Overall Confidence Statistics:\")\n",
    "    print(f\"  Mean confidence: {np.mean(all_confidences):.4f}\")\n",
    "    print(f\"  Median confidence: {np.median(all_confidences):.4f}\")\n",
    "    print(f\"  Standard deviation: {np.std(all_confidences):.4f}\")\n",
    "    print(f\"  Min confidence: {np.min(all_confidences):.4f}\")\n",
    "    print(f\"  Max confidence: {np.max(all_confidences):.4f}\")\n",
    "    \n",
    "    print(f\"\\n🎲 Uncertainty Analysis:\")\n",
    "    print(f\"  Mean uncertainty: {np.mean(all_uncertainties):.4f}\")\n",
    "    print(f\"  Median uncertainty: {np.median(all_uncertainties):.4f}\")\n",
    "    print(f\"  High uncertainty (>0.8): {sum(1 for u in all_uncertainties if u > 0.8)} images\")\n",
    "    \n",
    "    # Enhanced confidence distribution\n",
    "    very_high_conf = sum(1 for c in all_confidences if c > 0.95)\n",
    "    high_conf = sum(1 for c in all_confidences if 0.9 <= c <= 0.95)\n",
    "    medium_conf = sum(1 for c in all_confidences if 0.7 <= c < 0.9)\n",
    "    low_conf = sum(1 for c in all_confidences if c < 0.7)\n",
    "    \n",
    "    print(f\"\\n🎯 Enhanced Confidence Distribution:\")\n",
    "    print(f\"  Very high confidence (>0.95): {very_high_conf} images ({very_high_conf/total_predictions*100:.1f}%)\")\n",
    "    print(f\"  High confidence (0.9-0.95): {high_conf} images ({high_conf/total_predictions*100:.1f}%)\")\n",
    "    print(f\"  Medium confidence (0.7-0.9): {medium_conf} images ({medium_conf/total_predictions*100:.1f}%)\")\n",
    "    print(f\"  Low confidence (<0.7): {low_conf} images ({low_conf/total_predictions*100:.1f}%)\")\n",
    "    \n",
    "    # Create enhanced visualizations\n",
    "    print(\"\\n📊 Creating enhanced result visualizations...\")\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Confidence distribution histogram\n",
    "    ax1.hist(all_confidences, bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.axvline(np.mean(all_confidences), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(all_confidences):.3f}')\n",
    "    ax1.set_title('Prediction Confidence Distribution', fontweight='bold')\n",
    "    ax1.set_xlabel('Confidence')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Class distribution pie chart\n",
    "    ax2.pie(class_counts.values(), labels=[f'{k.upper()}\\n({v} images)' for k, v in class_counts.items()], \n",
    "            autopct='%1.1f%%', startangle=90, colors=['lightcoral', 'lightblue'])\n",
    "    ax2.set_title('Class Distribution', fontweight='bold')\n",
    "    \n",
    "    # Confidence vs Uncertainty scatter plot\n",
    "    confidences = [r['confidence'] for r in test_results]\n",
    "    uncertainties = [r['uncertainty'] for r in test_results]\n",
    "    colors = ['red' if r['prediction'] == 'yes' else 'blue' for r in test_results]\n",
    "    \n",
    "    ax3.scatter(confidences, uncertainties, c=colors, alpha=0.6, s=30)\n",
    "    ax3.set_xlabel('Confidence')\n",
    "    ax3.set_ylabel('Uncertainty')\n",
    "    ax3.set_title('Confidence vs Uncertainty', fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend for colors\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor='red', label='COVID-19 Positive'),\n",
    "                      Patch(facecolor='blue', label='COVID-19 Negative')]\n",
    "    ax3.legend(handles=legend_elements)\n",
    "    \n",
    "    # Confidence levels bar chart\n",
    "    conf_levels = ['Very High\\n(>0.95)', 'High\\n(0.9-0.95)', 'Medium\\n(0.7-0.9)', 'Low\\n(<0.7)']\n",
    "    conf_counts = [very_high_conf, high_conf, medium_conf, low_conf]\n",
    "    colors_bar = ['darkgreen', 'green', 'orange', 'red']\n",
    "    \n",
    "    bars = ax4.bar(conf_levels, conf_counts, color=colors_bar, alpha=0.7)\n",
    "    ax4.set_title('Confidence Level Distribution', fontweight='bold')\n",
    "    ax4.set_ylabel('Number of Images')\n",
    "    \n",
    "    for bar, count in zip(bars, conf_counts):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(conf_counts) * 0.01,\n",
    "                str(count), ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Save results\n",
    "    results_plot_path = os.path.join(RESULTS_PATH, 'plots', 'prediction_results_50epochs.png')\n",
    "    fig.savefig(results_plot_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✅ Results visualization saved to: {results_plot_path}\")\n",
    "    \n",
    "    # Save detailed results to JSON\n",
    "    import json\n",
    "    results_json_path = os.path.join(RESULTS_PATH, 'predictions', 'detailed_predictions_50epochs.json')\n",
    "    os.makedirs(os.path.dirname(results_json_path), exist_ok=True)\n",
    "    \n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    json_results = []\n",
    "    for result in test_results:\n",
    "        json_result = {k: float(v) if isinstance(v, (np.floating, np.integer)) else v \n",
    "                      for k, v in result.items()}\n",
    "        json_results.append(json_result)\n",
    "    \n",
    "    with open(results_json_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'metadata': {\n",
    "                'model_version': '50_epochs_enhanced',\n",
    "                'total_predictions': total_predictions,\n",
    "                'prediction_date': str(pd.Timestamp.now() if 'pd' in globals() else 'Unknown'),\n",
    "                'model_performance': {\n",
    "                    'final_val_accuracy': final_val_acc,\n",
    "                    'best_val_accuracy': best_val_acc\n",
    "                }\n",
    "            },\n",
    "            'predictions': json_results,\n",
    "            'summary_statistics': {\n",
    "                'class_distribution': class_counts,\n",
    "                'confidence_stats': {\n",
    "                    'mean': float(np.mean(all_confidences)),\n",
    "                    'median': float(np.median(all_confidences)),\n",
    "                    'std': float(np.std(all_confidences)),\n",
    "                    'min': float(np.min(all_confidences)),\n",
    "                    'max': float(np.max(all_confidences))\n",
    "                },\n",
    "                'confidence_levels': {\n",
    "                    'very_high': very_high_conf,\n",
    "                    'high': high_conf,\n",
    "                    'medium': medium_conf,\n",
    "                    'low': low_conf\n",
    "                }\n",
    "            }\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"✅ Detailed results saved to: {results_json_path}\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ No test results to display\")\n",
    "\n",
    "# =============================================================================\n",
    "# CELL 14: Final Summary and Advanced Recommendations\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING AND PREDICTION COMPLETED SUCCESSFULLY - 50 EPOCHS VERSION!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nFiles created during this session:\")\n",
    "print(f\"✅ Final trained model: {model_filename}\")\n",
    "print(f\"✅ Best model checkpoint: {os.path.join(MODEL_SAVE_PATH, 'best_model_50epochs.h5')}\")\n",
    "print(f\"✅ Enhanced training history: {history_filename}\")\n",
    "print(f\"✅ Model summary: {summary_filename}\")\n",
    "print(f\"✅ Training/validation directories: {PROCESSED_TRAIN_DIR}, {PROCESSED_VAL_DIR}\")\n",
    "\n",
    "if test_results:\n",
    "    print(f\"✅ Detailed predictions: {results_json_path}\")\n",
    "    print(f\"✅ Results visualization: {results_plot_path}\")\n",
    "\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"✅ Final training accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"✅ Final validation accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"✅ Best validation accuracy: {best_val_acc:.4f} (epoch {best_val_acc_epoch})\")\n",
    "print(f\"✅ Final validation loss: {final_val_loss:.4f}\")\n",
    "\n",
    "if test_results:\n",
    "    avg_confidence = np.mean([r['confidence'] for r in test_results])\n",
    "    high_conf_pct = (very_high_conf + high_conf) / total_predictions * 100\n",
    "    print(f\"✅ Test images processed: {len(test_results)}\")\n",
    "    print(f\"✅ Average prediction confidence: {avg_confidence:.4f}\")\n",
    "    print(f\"✅ High confidence predictions: {high_conf_pct:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED RECOMMENDATIONS AND NEXT STEPS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🎯 Model Performance Analysis:\")\n",
    "if final_val_acc > 0.9:\n",
    "    print(\"✅ Excellent model performance achieved!\")\n",
    "elif final_val_acc > 0.8:\n",
    "    print(\"✅ Good model performance. Consider fine-tuning for improvement.\")\n",
    "else:\n",
    "    print(\"⚠️ Model performance could be improved. See recommendations below.\")\n",
    "\n",
    "print(\"\\n🔧 For Further Improvement:\")\n",
    "print(\"1. Fine-tuning approach:\")\n",
    "print(\"   - Unfreeze top layers of VGG16 for domain-specific learning\")\n",
    "print(\"   - Use lower learning rate (1e-5) for fine-tuning\")\n",
    "print(\"   - Implement gradual unfreezing strategy\")\n",
    "\n",
    "print(\"\\n2. Data enhancement:\")\n",
    "print(\"   - Collect more diverse training data\")\n",
    "print(\"   - Implement advanced augmentation (mixup, cutmix)\")\n",
    "print(\"   - Consider synthetic data generation\")\n",
    "\n",
    "print(\"\\n3. Architecture experiments:\")\n",
    "print(\"   - Try EfficientNet, ResNet, or DenseNet backbones\")\n",
    "print(\"   - Implement ensemble methods\")\n",
    "print(\"   - Experiment with attention mechanisms\")\n",
    "\n",
    "print(\"\\n4. Advanced techniques:\")\n",
    "print(\"   - Implement class activation maps (CAM) for explainability\")\n",
    "print(\"   - Use test-time augmentation for robust predictions\")\n",
    "print(\"   - Apply uncertainty quantification methods\")\n",
    "\n",
    "print(\"\\n📊 Production Deployment:\")\n",
    "print(\"1. Model validation:\")\n",
    "print(\"   - Test on external datasets\")\n",
    "print(\"   - Conduct clinical validation studies\")\n",
    "print(\"   - Implement A/B testing framework\")\n",
    "\n",
    "print(\"\\n2. Implementation considerations:\")\n",
    "print(\"   - Set appropriate confidence thresholds\")\n",
    "print(\"   - Implement human-in-the-loop workflows\")\n",
    "print(\"   - Create model monitoring dashboards\")\n",
    "\n",
    "print(\"\\n3. Regulatory and ethical:\")\n",
    "print(\"   - Ensure HIPAA compliance for medical data\")\n",
    "print(\"   - Implement bias detection and mitigation\")\n",
    "print(\"   - Document model limitations and failure cases\")\n",
    "\n",
    "print(\"\\n🚀 Comparison with 10-epoch version:\")\n",
    "print(\"- Expected better generalization with 50-epoch training\")\n",
    "print(\"- More stable predictions due to extended training\")\n",
    "print(\"- Better feature learning from enhanced augmentation\")\n",
    "print(\"- Reduced overfitting with improved regularization\")\n",
    "\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
